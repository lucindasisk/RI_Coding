---
title: "Example_PenalizedRegression"
author: "Lucinda Sisk"
date: "9/19/2019"
output:
  pdf_document: default
  html_document: default
---


###Elastic Net


Elastic Net produces a regression model that is penalized with both the L1-norm and L2-norm. The consequence of this is to effectively shrink coefficients (like in ridge regression) and to set some coefficients to zero (as in LASSO).

Code derived from [**stack overflow - click for link **](https://stackoverflow.com/questions/47732633/extract-data-from-glmnet-output-data)


```{r}

stress <- read_csv('/Users/lucindasisk/Box/LS_Folders/CANDLab_LS/TraumaData/dummyData/meeting_dummy_data_10.10.19.csv')



library(tidyverse)
library(caret)
library(glmnet)

#Subset example data into predictor and outcome variables
stress_age_vars <- stress[2:56,2:31]
predictor_stress <- model.matrix(~., stress_age_vars)
outcome_stress <- stress$brain_dv_1[2:56] 

# Build the model using the training set
model_stress <- cv.glmnet(x = predictor_stress,
                        y = outcome_stress, 
                        family = 'gaussian',#if outcome is continuous, gaussian
                        alpha = .5, #alpha=0.5 *check
                        nfolds = 5, #number of crossfold validations you want to do,
                        standardize.response = TRUE,
                        intercept = FALSE)

# Best tuning parameter (smallest lambda)
(coef(model_stress, s="lambda.min")) #if you just run the cv.glmnet, will give you ~100 diff results, want to use tuning parameter that give you minimum square error. This gets the coefficient that gives you the minimum square error

#See best lambda
(model_stress$lambda[which.min(model_stress$cvm)]) #This saying waht will be the tuning parameter; not important to report for results

#### TEST MODEL ####
#Make testing model
test_model_stress <- glmnet(x = predictor_stress,
                            y = outcome_stress,
                            family="gaussian",
                            alpha=1,
                            intercept=FALSE)







(tmp_coeffs <- coef(model_stress, s = "lambda.min"))
dimnames(coef(model_stress)) #The second value in this vector will be linked to the predictors
#glmnet doen't automatically generate p-value
#Second package to plug in coefficients; this will give p-value

```


$\\$



Elastic Net produces a regression model that is penalized with both the L1-norm and L2-norm. The consequence of this is to effectively shrink coefficients (like in ridge regression) and to set some coefficients to zero (as in LASSO).

Example using [**stack overflow - click for link **](https://stackoverflow.com/questions/47732633/extract-data-from-glmnet-output-data)

```{r}
library(tidyverse)
library(caret)
library(glmnet)

#Subset example data into predictor and outcome variables
data(iris)
predictor_iris <- model.matrix(~., iris)[,2:5]
outcome_iris <- iris$Species %>%
    as_factor()

# Build the model using the training set
model_iris <- cv.glmnet(x = predictor_iris,
                        y = outcome_iris,
                      family = "multinomial", #if outcome is continuous, gaussian
                      alpha = 1, #alpha=0.5 *check
                      nfolds = 5, #number of crossfold validations you want to do
                      intercept = FALSE)

# Best tuning parameter (smallest lambda)
(coef(model_iris, s="lambda.min")) #if you just run the cv.glmnet, will give you ~100 diff results, want to use tuning parameter that give you minimum square error. This gets the coefficient that gives you the minimum square error

#See best lambda
(model_iris$lambda[which.min(model_iris$cvm)]) #This saying waht will be the tuning parameter; not important to report for results

#### TEST MODEL ####
#Make testing model
test_model_iris <- glmnet(x = predictor_iris,
                    y = outcome_iris,
                    family="multinomial",
                    alpha=1,
                    intercept=FALSE)

#Extract meaning from model... ?
plot(test_model_iris, xvar = "lambda") +
abline(v = log(model_iris$lambda[which.min(model_iris$cvm)]), col="purple")

```


Extract variable names [**from this Stack Overflow answer - click for link**](https://stackoverflow.com/questions/27801130/extracting-coefficient-variable-names-from-glmnet-into-a-data-frame)


```{r}
(tmp_coeffs <- coef(model_iris, s = "lambda.min"))
dimnames(coef(model_iris)) #The second value in this vector will be linked to the predictors
#glmnet doen't automatically generate p-value
#Second package to plug in coefficients; this will give p-value

```

$\\$




### Example using [**STHDA Resource - click for link**]( http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/#elastic-net )
```{r}
data("Boston", package="MASS")

# Split the data into training and test set
set.seed(123)
training.samples <- Boston$medv %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]

# Predictor variables
predictors <- model.matrix(medv~., train.data)[,-1]
# Outcome variable
outcomes <- train.data$medv

# Build the model using the training set
set.seed(123)
model_train <- train(
  medv ~., data = train.data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
)
# Best tuning parameter
model_train$bestTune

# Coefficient of the final model. You need
# to specify the best lambda
coef(model_train$finalModel, model_train$bestTune$lambda)

# Make predictions on the test data
x.test <- model.matrix(medv ~., test.data)[,-1]
predictions <- model_train %>% predict(x.test)
# Model performance metrics
data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  Rsquare = R2(predictions, test.data$medv)
)

# Extract meaning from model???
plot(model_train, xvar = "lambda")

# coeffs <- coef(model_train$finalModel, model_train$bestTune$lambda) %>%
#     tibble()

```

#### Questions

1. How do we match predictors to outcome variables? I.e. is there a place to specify that subject ID links the two samples or does it figure this out automatically?

2. Do we need to pull samples from the same dataframe, two different data frames, or does it not matter?

3. We tried modeling this as in an online tutorial, but got different parameters for $\alpha$ and $\lambda$ than the tutorial did. Is this attributable to different versions/a newer release of R? $\alpha$: 0.1 $\lambda$: 0.03875385. In example,  $\alpha$: 0.1 $\lambda$: 0.21.

4. What is the coef function (coef(model$finalModel, model$bestTune$lambda)) doing here? 



```{r}
# Unsuccessful attempt to replicate with our data

# example_data <- read_csv('/Users/lucindasisk/Box/LS_Folders/CANDLab_LS/TraumaData/CTQ_ROI_SCR_SOBP2019_data_set_04.28.19_FINAL.csv') 
# outcome_vars <- example_data %>%
#     select(Shapes2_Aminus) %>%
#     as_vector() %>%
#     replace_na(0) 
# 
# predictor_vars <- example_data %>%
#     select(Sex_Final, Age_Final,
#            Hindy_L_AntHipp_test1_ABminus_predot:Hindy_L_AntHipp_test1_AllShapes_dot) %>%
#     replace_na(list(x = 0, y = 0)) 
# 
# predictor_vars2 <- model.matrix( ~ ., predictor_vars)

```



