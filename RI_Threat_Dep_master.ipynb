{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and Code UCLA RI Data - Adult Version\n",
    "### Usage: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from glob import glob\n",
    "today = str(date.today())\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set path variables and read in data set\n",
    "user = os.path.expanduser('~')\n",
    "home = os.path.join(user, 'Box/LS_Folders/CANDLab_LS/TraumaData')\n",
    "out = os.path.join(home, 'LS_results')\n",
    "V1 = os.path.join(home,'RI_Downloads/V1_UCLARIADULT52218_DATA_2019-08-20_1332.csv')\n",
    "V2 = os.path.join(home,'RI_Downloads/V2_UCLARIADULT52218_DATA_2019-08-20_1332.csv')\n",
    "V3 = os.path.join(home,'RI_Downloads/V3_UCLARIADULT52218_DATA_2019-08-20_1333.csv')\n",
    "V4 = os.path.join(home,'RI_Downloads/V4_UCLARIADULT52218_DATA_2019-08-20_1334.csv')\n",
    "V5 = os.path.join(home,'RI_Downloads/V5_UCLARIADULT52218_DATA_2019-08-20_1334.csv')\n",
    "V6 = os.path.join(home,'RI_Downloads/V6_UCLARIADULT52218_DATA_2019-08-20_1335.csv')\n",
    "V7 = os.path.join(home,'RI_Downloads/V7_UCLARIADULT52218_DATA_2019-08-20_1335.csv')\n",
    "V8 = os.path.join(home,'RI_Downloads/V8_UCLARIADULT52218_DATA_2019-08-20_1336.csv')\n",
    "V9 = os.path.join(home,'RI_Downloads/V9_UCLARIADULT52218_DATA_2019-08-20_1354.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 = pd.read_csv(V1)\n",
    "# V2 = pd.read_csv(V2)\n",
    "# V3 = pd.read_csv(V3)\n",
    "# V4 = pd.read_csv(V4)\n",
    "# V5 = pd.read_csv(V5)\n",
    "# V6 = pd.read_csv(V6)\n",
    "# V7 = pd.read_csv(V7)\n",
    "# V8 = pd.read_csv(V8)\n",
    "# V9 = pd.read_csv(V9)\n",
    "\n",
    "\n",
    "\n",
    "# m1=pd.merge(V1,V2, on='ucla_a_id',how='outer')\n",
    "# m2=pd.merge(m1, V3, on='ucla_a_id',how='outer')\n",
    "# m3=pd.merge(m2, V4, on='ucla_a_id',how='outer')\n",
    "# m4=pd.merge(m3, V5, on='ucla_a_id',how='outer')\n",
    "# m5=pd.merge(m4, V6, on='ucla_a_id',how='outer')\n",
    "# m6=pd.merge(m5, V7, on='ucla_a_id',how='outer')\n",
    "# m7=pd.merge(m6, V8, on='ucla_a_id',how='outer')\n",
    "# m8=pd.merge(m7, V9, on='ucla_a_id',how='outer')\n",
    "m85 = m8.head()\n",
    "m85.to_csv(os.path.join(home, 'all_vs_test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clean Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols(df):\n",
    "    empty_cols = []\n",
    "    for col_num in range(4, len(df.columns)):\n",
    "        try:\n",
    "            col = df.iloc[:,col_num].dropna().astype(int)\n",
    "            if col.sum() < 1 :\n",
    "                empty_cols.append(col_num)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('couldnt index column {}'.format(col_num))\n",
    "    return empty_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to drop empty columns and replace 999 with NaN (get_cols() function nested within)\n",
    "def clean_cols(path, version):\n",
    "    print('working on {}'.format(version))\n",
    "    dset = pd.read_csv(path, header = 0)\n",
    "    dset = dset.replace('999',np.nan)\n",
    "    dset = dset.replace(999,np.nan)\n",
    "    dset_clean = dset.dropna(axis=1, how='all')\n",
    "    empty_cols =get_cols(dset_clean)\n",
    "    dset_clean_empty = dset_clean.drop(dset_clean.columns[empty_cols], axis=1).dropna(axis=1, how='all')   \n",
    "    return dset_clean_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on v1\n",
      "working on v2\n",
      "working on v3\n",
      "working on v4\n",
      "working on v5\n",
      "working on v6\n",
      "working on v7\n",
      "working on v8\n",
      "working on v9\n",
      "couldnt index column 101\n",
      "couldnt index column 103\n",
      "couldnt index column 294\n",
      "couldnt index column 295\n",
      "couldnt index column 296\n"
     ]
    }
   ],
   "source": [
    "#Run function to clean all versions\n",
    "v1_clean = clean_cols(V1, 'v1')\n",
    "v2_clean = clean_cols(V2, 'v2')\n",
    "v3_clean = clean_cols(V3, 'v3')\n",
    "v4_clean = clean_cols(V4, 'v4')\n",
    "v5_clean = clean_cols(V5, 'v5')\n",
    "v6_clean = clean_cols(V6, 'v6')\n",
    "v7_clean = clean_cols(V7, 'v7')\n",
    "v8_clean = clean_cols(V8, 'v8')\n",
    "v9_clean = clean_cols(V9, 'v9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge cleaned versions together\n",
    "m1=pd.merge(v1_clean,v2_clean, on='ucla_a_id',how='outer')\n",
    "m2=pd.merge(m1, v3_clean, on='ucla_a_id',how='outer')\n",
    "m3=pd.merge(m2, v4_clean, on='ucla_a_id',how='outer')\n",
    "m4=pd.merge(m3, v5_clean, on='ucla_a_id',how='outer')\n",
    "m5=pd.merge(m4, v6_clean, on='ucla_a_id',how='outer')\n",
    "m6=pd.merge(m5, v7_clean, on='ucla_a_id',how='outer')\n",
    "m7=pd.merge(m6, v8_clean, on='ucla_a_id',how='outer')\n",
    "m8=pd.merge(m7, v9_clean, on='ucla_a_id',how='outer')\n",
    "\n",
    "final=m8\n",
    "final.to_csv(os.path.join(out, 'All_UCLA_RI_versions_Merged_Cleaned_{}.csv'.format(today)), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Code Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boilerplate code to parallelize operations\n",
    "def parallelize_dataframe(df, func, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create threat and dep variables\n",
    "thr_cols = [col for col in final.columns if 'icthr' in col]\n",
    "dep_cols = [col for col in final.columns if 'icdep' in col]\n",
    "thr_cols.append('ucla_a_id')\n",
    "dep_cols.append('ucla_a_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select columns from data frame; create final_summed variables\n",
    "df_thr = final[thr_cols]\n",
    "df_dep = final[dep_cols]\n",
    "df_thr['num_thr_ev'] = df_thr.sum(axis=1)\n",
    "df_dep['num_dep_ev'] = df_dep.sum(axis=1)\n",
    "final_summed_thr = df_thr[['ucla_a_id','num_thr_ev']]\n",
    "final_summed_dep = df_dep[['ucla_a_id','num_dep_ev']]\n",
    "final_summed=pd.merge(final_summed_thr, final_summed_dep, on='ucla_a_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up loops to create new dfs by age\n",
    "events = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','x','y']\n",
    "numoccs = range(1,32) #number of occurrences -- up to 31 for each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty lists to append threat/dep scores\n",
    "thr_df = []\n",
    "dep_df = []\n",
    "all_df = []\n",
    "\n",
    "#Define function to count number of endorsements of threat or deprivation\n",
    "#Where e is events and x is numoccs\n",
    "def score_num(dset, e, x, thde): \n",
    "    data = dset.iloc[i]\n",
    "    print('Working on row {}'.format(i))\n",
    "    try:\n",
    "        if thde == 'icthr':\n",
    "            if data['ucla_a_{}_e{}_{}'.format(e, x, thde)] == int('1'):\n",
    "                subthr = str(data['ucla_a_id'])\n",
    "                itemthr = str(data['ucla_a_{}_e{}_{}'.format(e, x, thde)])\n",
    "                asevthr = str(data['ucla_a_{}_e{}_avgsev'.format(e, x)])\n",
    "                wsevthr =  str(data['ucla_a_{}_e{}_worsev'.format(e, x)])\n",
    "                agethr = str(data['ucla_a_{}_e{}_age'.format(e, x)])\n",
    "                print('appending data for {}'.format(subthr))\n",
    "                thr_df.append([subthr, agethr, itemthr, asevthr, wsevthr])\n",
    "            else:\n",
    "                pass\n",
    "        elif thde == 'icdep':\n",
    "            if data['ucla_a_{}_e{}_{}'.format(e, x, thde)] == int('1'):\n",
    "                subdep = str(data['ucla_a_id'])\n",
    "                itemdep = str(data['ucla_a_{}_e{}_{}'.format(e, x, thde)])\n",
    "                asevdep = str(data['ucla_a_{}_e{}_avgsev'.format(e, x)])\n",
    "                wsevdep =  str(data['ucla_a_{}_e{}_worsev'.format(e, x)])\n",
    "                agedep = str(data['ucla_a_{}_e{}_age'.format(e, x)])\n",
    "                print('appending data for {}'.format(subdep))\n",
    "                dep_df.append([subdep, agedep, itemdep, asevdep, wsevdep])\n",
    "            else:\n",
    "                pass\n",
    "        elif thde == 'all':\n",
    "            if data['ucla_a_{}_e{}_cod'.format(e, x, thde)] == int('1'):\n",
    "                suball = str(data['ucla_a_id'])\n",
    "                itemall = str(data['ucla_a_{}_e{}_cod'.format(e, x, thde)])\n",
    "                asevall = str(data['ucla_a_{}_e{}_avgsev'.format(e, x)])\n",
    "                wsevall = str(data['ucla_a_{}_e{}_worsev'.format(e, x)])\n",
    "                ageall = str(data['ucla_a_{}_e{}_age'.format(e, x)])\n",
    "                print('appending data for {}'.format(suball))\n",
    "                all_df.append([suball, ageall, itemall, asevall, wsevall])\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform actual counts\n",
    "for i in range(0,len(final)):\n",
    "    for e in events:\n",
    "        for x in numoccs:\n",
    "            score_num(final,e,x,'icthr')\n",
    "            score_num(final,e,x,'icdep')\n",
    "            score_num(final,e,x,'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put counts into dataframes\n",
    "thr_data = pd.DataFrame(thr_df).rename(columns={0:'ucla_a_id',1:'age_at_occ',2:'endorse_thr',3:'avg_sev',4:'worst_sev'})   \n",
    "dep_data = pd.DataFrame(dep_df).rename(columns={0:'ucla_a_id',1:'age_at_occ',2:'endorse_dep',3:'avg_sev',4:'worst_sev'})   \n",
    "all_data = pd.DataFrame(all_df).rename(columns={0:'ucla_a_id',1:'age_at_occ',2:'endorse_any',3:'avg_sev',4:'worst_sev'})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data to CSV for checking\n",
    "thr_data.to_csv(os.path.join(out,'Cleaned_threat_endorsements_{}.csv'.format(today)), index=False)\n",
    "dep_data.to_csv(os.path.join(out,'Cleaned_dep_endorsements_{}.csv'.format(today)), index=False)\n",
    "all_data.to_csv(os.path.join(out,'Cleaned_all_endorsements_{}.csv'.format(today)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IF ABOVE ALREAD RUN: READ IN DFs TO SAVE TIME\n",
    "\n",
    "thr_data = pd.read_csv(os.path.join(out,'Cleaned_threat_endorsements_2019-08-20.csv'))\n",
    "dep_data = pd.read_csv(os.path.join(out,'Cleaned_dep_endorsements_2019-08-20.csv'))\n",
    "all_data = pd.read_csv(os.path.join(out,'Cleaned_all_endorsements_2019-08-20.csv'))\n",
    "\n",
    "thr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset dataframes for summing and aggregation\n",
    "thr_data1=thr_data.set_index('ucla_a_id').astype(float)\n",
    "dep_data1=dep_data.set_index('ucla_a_id').astype(float)\n",
    "all_data1=all_data.set_index('ucla_a_id').astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform threat endorsements\n",
    "thr_wide = thr_data1.pivot_table(index='ucla_a_id', \n",
    "                                columns='age_at_occ', \n",
    "                                values=['endorse_thr',\n",
    "                                        'avg_sev',\n",
    "                                        'worst_sev'],\n",
    "                               aggfunc=np.sum)    \n",
    "\n",
    "#Transform all endorsements\n",
    "dep_wide = dep_data1.pivot_table(index='ucla_a_id', \n",
    "                                columns='age_at_occ', \n",
    "                                values=['endorse_dep',\n",
    "                                        'avg_sev',\n",
    "                                        'worst_sev'],\n",
    "                               aggfunc=np.sum)    \n",
    "\n",
    "#Transform all endorsements\n",
    "all_wide = all_data1.pivot_table(index='ucla_a_id', \n",
    "                                columns='age_at_occ', \n",
    "                                values=['endorse_any',\n",
    "                                        'avg_sev',\n",
    "                                        'worst_sev'],\n",
    "                               aggfunc=np.sum)    \n",
    "\n",
    "#np.sum() function implemented above sums two NaNs to 0.0, which is inaccurate. \n",
    "#This function replaces 0s with NaNs again in the resultant data frames for the severity variables.\n",
    "\n",
    "def replace_0s(df):\n",
    "    df['avg_sev'] = df['avg_sev'].replace(0, np.nan)\n",
    "    df['worst_sev'] = df['worst_sev'].replace(0, np.nan)\n",
    "    return df\n",
    "\n",
    "thr_wide=replace_0s(thr_wide)\n",
    "dep_wide=replace_0s(dep_wide)\n",
    "all_wide=replace_0s(all_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wide dataframes to CSV for analysis\n",
    "thr_wide.to_csv(os.path.join(out, 'Cleaned_WIDE_threat_endorsements_{}.csv'.format(today)))\n",
    "dep_wide.to_csv(os.path.join(out, 'Cleaned_WIDE_dep_endorsements_{}.csv'.format(today)))\n",
    "all_wide.to_csv(os.path.join(out, 'Cleaned_WIDE_any_endorsements_{}.csv'.format(today)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  # Code Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "data=dset_cleaned.iloc[3]\n",
    "x = str(data['ucla_a_id'])\n",
    "d.append(x)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_data = pd.DataFrame(thr_df)\n",
    "thr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threat_df1 = pd.DataFrame(thr_df).groupby(0).sum().reset_index()\n",
    "threat_df= threat_df1.rename(columns={0:'ucla_a_id', 1:\"num_threat\"})\n",
    "\n",
    "depriv_df1 = pd.DataFrame(dep_df).groupby(0).sum().reset_index()\n",
    "depriv_df = depriv_df1.rename(columns={0:'ucla_a_id', 1:\"num_dep\"})\n",
    "\n",
    "merged_df = pd.merge(threat_df, depriv_df, on='ucla_a_id', how='outer')\n",
    "\n",
    "finaldf = pd.merge(merged_df, dset, on='ucla_a_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(os.path.join(home, 'LS_scored_traumaData_'+today+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create thr/dep by age function\n",
    "pairs=[]\n",
    "def create_age_exp_pairs(df, typ_exp):\n",
    "    criteria1 = df.columns.str.contains('age')\n",
    "    criteria2 = df.columns.str.contains('ic{}'.format(typ_exp))\n",
    "    criteria_all = criteria1 | criteria2\n",
    "    cols = df.columns[criteria_all]\n",
    "    new_df=df[cols]\n",
    "    return new_df\n",
    "\n",
    "#Create thr/dep by age data frames\n",
    "thr_age_df = create_age_exp_pairs(dset_cleaned, 'thr').dropna(axis=1, how='all')\n",
    "dep_age_df = create_age_exp_pairs(dset_cleaned, 'dep').dropna(axis=1, how='all')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
