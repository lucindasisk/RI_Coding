---
title: "Example_PenalizedRegression"
author: "Lucinda Sisk"
date: "9/19/2019"
output:
  pdf_document: default
  html_document: default
---



```{r, tidy=TRUE, include=FALSE}
library(tidyverse)
library(caret)
library(glmnet)

install.packages("selectiveInference")
library(selectiveInference)

stress <- read.csv('~/Box Sync/PhD/Grants/NRSA/Pilot_Analyses/RI_Outputs/Cleaned_WIDE_any_endorsements_2019-11-19_final.csv') 

ROI <- read.csv('~/Box Sync/PhD/Projects/Shapes/Shapes_RI/Data/F30_pilot_ROI_RI_data_mod_1.21.20.csv')

stress_ROI <- merge(stress, ROI, by = "subjectid")

```

### Random Forest Modeling

"Random Forest is an ensemble machine learning technique capable of performing both regression and classification tasks using multiple decision trees and a statistical technique called bagging. Bagging along with boosting are two of the most popular ensemble techniques which aim to tackle high variance and high bias. - [Link here](https://gdcoder.com/random-forest-regressor-explained-in-depth)"

Helpful conceptual resorce: [Towards Data Science](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)

Random Forest in R Tutorial: [Link Here](https://towardsdatascience.com/random-forest-in-r-f66adf80ec9)

R-bloggers Random Forest in depth:[Link Here](https://www.r-bloggers.com/random-forests-in-r/)

Berkeley Stats Information Page: [Link Here](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)

A Random Forest Guided Tour: Introductory paper [Link Here](https://www.normalesup.org/~scornet/paper/test.pdf)

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
library(randomForest)
require(caTools)

#Check that variables in dummy dataset are coded correctly
sapply(stress, class)

#Remove first row (only has ages)
stress <- stress[2:length(stress),]

#Recode sex as factor; rest are correct
stress <- stress %>%
  transform(sex=as.factor(sex))

#Split data into training and testing]
sample <-  sample.split(stress$ucla_a_id, SplitRatio = .75)

train <- subset(stress, sample == TRUE) %>%
  dplyr::select(-c('ucla_a_id','brain_dv_2':'brain_dv_4')) %>%
  data.frame() %>%
  na.omit()

test <- subset(stress, sample == FALSE) %>%
  dplyr::select(-c('ucla_a_id','brain_dv_2':'brain_dv_4')) %>%
  data.frame() %>%
  na.omit()

dim(train)
dim(test)


#Next, we initialize an instance of the randomForest class. Unlike scikit-learn, we don’t need to explicily call the fit method to train our model.
rf_mod <- randomForest(brain_dv_1 ~., 
                   data=train)

#View output
rf_mod

#plot
plot(rf_mod)

#By default, the number of decision trees in the forest is 500 and the number of features used as potential candidates for each split is 3. The model will automatically attempt to classify each of the samples in the Out-Of-Bag dataset and display a confusion matrix with the results.

#Now, we use our model to predict whether the people in our testing set have heart disease.
(pred = predict(rf_mod, newdata=test[-31]))


```

## Example from internet: [Link Here](https://towardsdatascience.com/random-forest-in-r-f66adf80ec9)
```{r, tidy = TRUE, tidy.opts=list(width.cutoff=60)}

#Random Forest Example

library(randomForest)
require(caTools)

#Download data
data <- read.csv(
  "~/Downloads/processed.cleveland.data",
  header=FALSE
)

#Check data dimensions
dim(data)

#Specify column names
names(data) <- c("age", "sex", "cp", "trestbps", "choi", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thai", "num")

#Check data
head(data)

# To simplify the problem, we’re only going to attempt to distinguish the presence of heart disease (values 1,2,3,4) from absence of heart disease (value 0). Therefore, we replace all labels greater than 1 by 1.
data$num[data$num > 1] <- 1

#Check summary data to make sure categorical variables are being correctly classified
summary(data)

#View type of each column
sapply(data, class)

#As we can see, sex is incorrectly treated as a number when in reality it can only be 1 if male and 0 if female. We can use the transform method to change the in built type of each feature.

data <- transform(
  data,
  age=as.integer(age),
  sex=as.factor(sex),
  cp=as.factor(cp),``
  trestbps=as.integer(trestbps),
  choi=as.integer(choi),
  fbs=as.factor(fbs),
  restecg=as.factor(restecg),
  thalach=as.integer(thalach),
  exang=as.factor(exang),
  oldpeak=as.numeric(oldpeak),
  slope=as.factor(slope),
  ca=as.factor(ca),
  thai=as.factor(thai),
  num=as.factor(num)
)
sapply(data, class)

#Now, the categorical variables are expressed as the counts for each respective class. The ca and thai of certain samples are ? indicating missing values. R expects missing values to be written as NA. After replacing them, we can use the colSums function to view the missing value counts of each column.

data[ data == "?"] <- NA
colSums(is.na(data))

#we’re just going to replace the missing values for thai with what is considered normal. Next, we’re going to drop the rows where ca is missing.
data$thai[which(is.na(data$thai))] <- as.factor("3.0")
data <- data[!(data$ca %in% c(NA)),]
colSums(is.na(data))

#If we run summary again, we’ll see that it still views ? as a potential class.
summary(data)

#To get around this issue, we cast the columns to factors.
data$ca <- factor(data$ca)
data$thai <- factor(data$thai)
summary(data)

#We’re going to set a portion of our data aside for testing.
sample = sample.split(data$num, SplitRatio = .75)
train = subset(data, sample == TRUE)
test  = subset(data, sample == FALSE)
dim(train)
dim(test)

#Next, we initialize an instance of the randomForest class. Unlike scikit-learn, we don’t need to explicily call the fit method to train our model.
rf_modelStress <- randomForest(
  num ~ .,
  data=train
)

#By default, the number of decision trees in the forest is 500 and the number of features used as potential candidates for each split is 3. The model will automatically attempt to classify each of the samples in the Out-Of-Bag dataset and display a confusion matrix with the results.

#Now, we use our model to predict whether the people in our testing set have heart disease.

pred = predict(rf_modelStress, newdata=test[-14])
pred
# Since this is a classification problem, we use a confusion matrix to evaluate the performance of our model. Recall that values on the diagonal correspond to true positives and true negatives (correct predictions) whereas the others correspond to false positives and false negatives.
(cm = table(test[,14], pred))

```

### Elastic Net


Elastic Net produces a regression model that is penalized with both the L1-norm and L2-norm. The consequence of this is to effectively shrink coefficients (like in ridge regression) and to set some coefficients to zero (as in LASSO).

Code derived from [**stack overflow - click for link **](https://stackoverflow.com/questions/47732633/extract-data-from-glmnet-output-data)



```{r}
#Subset example data into predictor and outcome variables
stress_ROI_drop <- stress_ROI[complete.cases(stress_ROI[ , 408]),]
stress_age_vars <- stress_ROI_drop[c(94:123)] #%>% na.omit()
options(na.action= "na.pass")
predictor_stress <- model.matrix(~., stress_age_vars) #%>% na.omit()
predictor_stress[is.na(predictor_stress)] <- 0
#covariate_vars <- stress[["sex"]][2:length(stress)]
#covariate_matrix <- model.matrix(covariate_vars)
outcome_stress <- stress_ROI[,407] %>% na.omit()

#Set Penalty factors for sex (https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.pdf)
pfac <- rep(1,32)
pfac[c(32)] = 0 

# Build the model using the training set
model_stress <- cv.glmnet(x = predictor_stress,
                          y = outcome_stress, 
                          penalty.factor = pfac,
                          family = 'gaussian', #if outcome is continuous, gaussian
                          alpha = .5, #alpha=0.5 *check
                          nfolds = 10, #number of crossfold validations you want to do,
                          intercept = FALSE)

# Best tuning parameter (smallest lambda)
(coef(model_stress, s="lambda.min")) #if you just run the cv.glmnet, will give you ~100 diff results, want to use tuning parameter that give you minimum square error. This gets the coefficient that gives you the minimum square error

#See best lambda
best_lambda_test <- (model_stress$lambda[which.min(model_stress$cvm)]) #This is saying what will be the tuning parameter; not important to report for results
best_lambda_test

#### TEST MODEL ####
#Make testing model
test_model_stress <- glmnet(x = predictor_stress,
                            y = outcome_stress,
                            penalty.factor = pfac,
                            lambda = best_lambda_test,
                            family="gaussian",
                            alpha=0.5,
                            intercept=FALSE)


(tmp_coeffs <- coef(test_model_stress, s = "lambda.min"))
dimnames(coef(test_model_stress)) #The second value in this vector will be linked to the predictors
#glmnet doen't automatically generate p-value
#Second package to plug in coefficients; this will give p-value

tmp_coeffs

#Plot model
plot(test_model_stress, xvar="lambda", label=TRUE)

#Fixed lasso inference
predictor_stress_no_int <- predictor_stress[,-c(1)]

predictor_stress_no_int

fixedLassoInf(predictor_stress_no_int,
              outcome_stress,
              beta,
              best_lambda_test,
              family = ("gaussian"),
              intercept=FALSE,
              add.targets=NULL,
              status=NULL,
              sigma=NULL,
              alpha=0.5,
              type=c("partial"),
              tol.beta=1e-5,
              tol.kkt=0.1,
              gridrange=c(-100,100),
              bits=NULL,
              verbose=FALSE,
              linesearch.try=10)

beta <- tmp_coeffs[-c(1:2),]
beta

predictor_stress

## Linear models (we can't interpret coefficients from elastic net: see https://stats.stackexchange.com/questions/274813/interpretation-of-positive-negative-and-magnitude-of-coefficients-in-lasso)

m1 <- lm(brain_dv_1 ~ endorse_any_12 + sex, stress)
summary(m1)

m2 <- lm(brain_dv_1 ~ endorse_any_13 + sex, stress)
summary(m2)

m3 <- lm(brain_dv_1~endorse_any_17 + sex, stress)
summary(m3)

m_allpreds <- lm(brain_dv_1 ~ endorse_any_12 + endorse_any_13 + endorse_any_17 + sex,
                 stress)
summary(m_allpreds)
#plot(x=stress$endorse_any_17, y=stress$brain_dv_1, xlim=c(0,5))
```

```{r}
stress_sum <- stress %>%
  select(-c('ucla_a_id')) %>%
  summarise_all(funs(sum)) %>%
  as.numeric()
  #add_column('ucla_a_id' = 'mean', .before=TRUE)

# stress_only <- stress %>%
#   rbind(stress_sum)

#stress_sum <- t(data.frame(stress_sum))
names=c(0:30)
barplot(stress_sum,  cex.axis=1)

```

```{r}
#Bootstrapping to create null distribution and test coefficient significance
#Subset example data into predictor and outcome variables

stress_age_vars <- stress[2:56,2:31]
predictor_stress <- model.matrix(~., stress_age_vars)
outcome_stress <- stress$brain_dv_1[2:56] 


null_dist <- NULL
for (i in 1:10){
   
  #Randomize variables
  stress_age_rand <- stress_age_vars[sample(1:56, 56, replace = FALSE), ] # Fix this so randomizes whole table
  outcome_stress_rand <- sample(stress$brain_dv_1[2:56] )
  predictor_stress_rand <- model.matrix(~., stress_age_rand)
  
  #Rerun test model
  model_stress_rand <- cv.glmnet(x = predictor_stress_rand,
                        y = outcome_stress_rand, 
                        family = 'gaussian',#if outcome is continuous, gaussian
                        alpha = .5, #alpha=0.5 *check
                        nfolds = 10, #number of crossfold validations you want to do,
                        intercept = FALSE)
  
  rand_coefs <- (coef(model_stress_rand, s="lambda.min"))
  rand_coefs_dropped <- tibble(lapply(rand_coefs@x, function(x) {x[x!=0]}))
  null_dist[[i]] <- rand_coefs_dropped
  
}
  

hist(null_dist,
     nclass=50)





# Build the model using the training set


# Best tuning parameter (smallest lambda)
 #if you just run the cv.glmnet, will give you ~100 diff results, want to use tuning parameter that give you minimum square error. This gets the coefficient that gives you the minimum square error




```






# Examples from the internet

$\\$



Elastic Net produces a regression model that is penalized with both the L1-norm and L2-norm. The consequence of this is to effectively shrink coefficients (like in ridge regression) and to set some coefficients to zero (as in LASSO).

Example using [**stack overflow - click for link **](https://stackoverflow.com/questions/47732633/extract-data-from-glmnet-output-data)

```{r}
library(tidyverse)
library(caret)
library(glmnet)

#Subset example data into predictor and outcome variables
data(iris)
predictor_iris <- model.matrix(~., iris)[,2:5]
outcome_iris <- iris$Species %>%
    as_factor()

# Build the model using the training set
model_iris <- cv.glmnet(x = predictor_iris,
                        y = outcome_iris,
                      family = "multinomial", #if outcome is continuous, gaussian
                      alpha = 1, #alpha=0.5 *check
                      nfolds = 5, #number of crossfold validations you want to do
                      intercept = FALSE)

# Best tuning parameter (smallest lambda)
(coef(model_iris, s="lambda.min")) #if you just run the cv.glmnet, will give you ~100 diff results, want to use tuning parameter that give you minimum square error. This gets the coefficient that gives you the minimum square error

#See best lambda
(model_iris$lambda[which.min(model_iris$cvm)]) #This saying waht will be the tuning parameter; not important to report for results

#### TEST MODEL ####
#Make testing model
test_model_iris <- glmnet(x = predictor_iris,
                    y = outcome_iris,
                    family="multinomial",
                    alpha=1,
                    intercept=FALSE)

#Extract meaning from model... ?
plot(test_model_iris, xvar = "lambda", label = TRUE) +
abline(v = log(model_iris$lambda[which.min(model_iris$cvm)]), col="purple")

```

#Office hrs: replicate by splitting data in half multiple times (random sampling)
#Do both that and p-value testing
  * Get residuals from test set; check if it is better than real set; try leave one out testign
  * Check if y-yhat is better for real data than when you shuffle the data


Extract variable names [**from this Stack Overflow answer - click for link**](https://stackoverflow.com/questions/27801130/extracting-coefficient-variable-names-from-glmnet-into-a-data-frame)


```{r}
(tmp_coeffs <- coef(model_iris, s = "lambda.min"))
dimnames(coef(model_iris)) #The second value in this vector will be linked to the predictors
#glmnet doen't automatically generate p-value
#Second package to plug in coefficients; this will give p-value

```

$\\$




### Example using [**STHDA Resource - click for link**]( http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/#elastic-net )
```{r}
data("Boston", package="MASS")

# Split the data into training and test set
set.seed(123)
training.samples <- Boston$medv %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]

# Predictor variables
predictors <- model.matrix(medv~., train.data)[,-1]
# Outcome variable
outcomes <- train.data$medv

# Build the model using the training set
set.seed(123)
model_train <- train(
  medv ~., data = train.data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
)
# Best tuning parameter
model_train$bestTune

# Coefficient of the final model. You need
# to specify the best lambda
coef(model_train$finalModel, model_train$bestTune$lambda)

# Make predictions on the test data
x.test <- model.matrix(medv ~., test.data)[,-1]
predictions <- model_train %>% predict(x.test)
# Model performance metrics
data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  Rsquare = R2(predictions, test.data$medv)
)

# Extract meaning from model???
plot(model_train, xvar = "lambda")

# coeffs <- coef(model_train$finalModel, model_train$bestTune$lambda) %>%
#     tibble()

```

#### Questions

1. How do we match predictors to outcome variables? I.e. is there a place to specify that subject ID links the two samples or does it figure this out automatically?

2. Do we need to pull samples from the same dataframe, two different data frames, or does it not matter?

3. We tried modeling this as in an online tutorial, but got different parameters for $\alpha$ and $\lambda$ than the tutorial did. Is this attributable to different versions/a newer release of R? $\alpha$: 0.1 $\lambda$: 0.03875385. In example,  $\alpha$: 0.1 $\lambda$: 0.21.

4. What is the coef function (coef(model$finalModel, model$bestTune$lambda)) doing here? 



```{r}
# Unsuccessful attempt to replicate with our data

# example_data <- read_csv('/Users/lucindasisk/Box/LS_Folders/CANDLab_LS/TraumaData/CTQ_ROI_SCR_SOBP2019_data_set_04.28.19_FINAL.csv') 
# outcome_vars <- example_data %>%
#     select(Shapes2_Aminus) %>%
#     as_vector() %>%
#     replace_na(0) 
# 
# predictor_vars <- example_data %>%
#     select(Sex_Final, Age_Final,
#            Hindy_L_AntHipp_test1_ABminus_predot:Hindy_L_AntHipp_test1_AllShapes_dot) %>%
#     replace_na(list(x = 0, y = 0)) 
# 
# predictor_vars2 <- model.matrix( ~ ., predictor_vars)

```




